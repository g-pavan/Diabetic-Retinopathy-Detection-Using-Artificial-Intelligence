{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO-Kg2vjAltz"
      },
      "source": [
        "# Step - 1\n",
        "\n",
        "## Importing Required Data Sets and Libraries\n",
        "- Pandas\n",
        "- Numpy\n",
        "- Matplotlib\n",
        "- seaborn\n",
        "- sklearn\n",
        "- PIL\n",
        "- keras\n",
        "- tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf_7805PvGgZ"
      },
      "source": [
        "# Import the necessary packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import random\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import display\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeCYdND-6CFt"
      },
      "source": [
        "# Set path to data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_EUHCz30v79"
      },
      "source": [
        "# Check the number of images in the dataset\n",
        "train = []\n",
        "label = []\n",
        "\n",
        "# os.listdir returns the list of files in the folder, in this case image class names\n",
        "for i in os.listdir('./train'):\n",
        "  train_class = os.listdir(os.path.join('train', i))\n",
        "  for j in train_class:\n",
        "    img = os.path.join('train', i, j)\n",
        "    train.append(img)\n",
        "    label.append(i)\n",
        "\n",
        "print('Number of train images : {} \\n'.format(len(train)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSmDlTCJM8q8"
      },
      "source": [
        "sns.countplot(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT3pMUedJ1YW"
      },
      "source": [
        "# Step - 2\n",
        "\n",
        "## PERFORM DATA EXPLORATION AND DATA VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bw_N3GC2GPi"
      },
      "source": [
        "# Visualize 5 images for each class in the dataset\n",
        "\n",
        "fig, axs = plt.subplots(5, 5, figsize = (20, 20))\n",
        "count = 0\n",
        "for i in os.listdir('./train'):\n",
        "  # get the list of images in a given class\n",
        "  train_class = os.listdir(os.path.join('train', i))\n",
        "  # plot 5 images per class\n",
        "  for j in range(5):\n",
        "    img = os.path.join('train', i, train_class[j])\n",
        "    img = PIL.Image.open(img)\n",
        "    axs[count][j].title.set_text(i)\n",
        "    axs[count][j].imshow(img)  \n",
        "  count += 1\n",
        "\n",
        "fig.tight_layout()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIa7-PfI64qm"
      },
      "source": [
        "# check the number of images in each class in the training dataset\n",
        "\n",
        "No_images_per_class = []\n",
        "Class_name = []\n",
        "for i in os.listdir('./train'):\n",
        "  train_class = os.listdir(os.path.join('train', i))\n",
        "  No_images_per_class.append(len(train_class))\n",
        "  Class_name.append(i)\n",
        "  print('Number of images in {} = {} \\n'.format(i, len(train_class)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fbGxuF47cdB"
      },
      "source": [
        "retina_df = pd.DataFrame({'Image': train,'Labels': label})\n",
        "retina_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXCkmtqPNLqQ"
      },
      "source": [
        "No_images_per_class\n",
        "Class_name\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(No_images_per_class, labels = Class_name, autopct = '%1.1f%%')\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w19AU_MxKBe2"
      },
      "source": [
        "# Step - 3\n",
        "\n",
        "## PERFORM DATA AUGMENTATION AND CREATE DATA GENERATOR\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XujDQf07laq"
      },
      "source": [
        "# Shuffle the data and split it into training and testing\n",
        "retina_df = shuffle(retina_df)\n",
        "train, test = train_test_split(retina_df, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBF1cWrD7qzO"
      },
      "source": [
        "# Create run-time augmentation on training and test dataset\n",
        "# For training datagenerator, we add normalization, shear angle, zooming range and horizontal flip\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale = 1./255,\n",
        "        shear_range = 0.2,\n",
        "        validation_split = 0.2, \n",
        "        horizontal_flip=True)\n",
        "\n",
        "# For test datagenerator, we only normalize the data.\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibd-9ob77udF"
      },
      "source": [
        "# Creating datagenerator for training, validation and test dataset.\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train,\n",
        "    directory='./',\n",
        "    x_col=\"Image\",\n",
        "    y_col=\"Labels\",\n",
        "    target_size=(256, 256),\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=32,\n",
        "    subset='training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_dataframe(\n",
        "    train,\n",
        "    directory='./',\n",
        "    x_col=\"Image\",\n",
        "    y_col=\"Labels\",\n",
        "    target_size=(256, 256),\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=32,\n",
        "    subset='validation')\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    test,\n",
        "    directory='./',\n",
        "    x_col=\"Image\",\n",
        "    y_col=\"Labels\",\n",
        "    target_size=(256, 256),\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3QN9Xm9KRDg"
      },
      "source": [
        "# Step - 4\n",
        "## BUILD RES-BLOCK BASED DEEP LEARNING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA12qNLM72sU"
      },
      "source": [
        "def res_block(X, filter, stage):\n",
        "  \n",
        "  # Convolutional_block\n",
        "  X_copy = X\n",
        "\n",
        "  f1 , f2, f3 = filter\n",
        "    \n",
        "  # Main Path\n",
        "  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_conv_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = MaxPool2D((2,2))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_a')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_conv_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_b')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_c')(X)\n",
        "\n",
        "\n",
        "  # Short path\n",
        "  X_copy = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_copy', kernel_initializer= glorot_uniform(seed = 0))(X_copy)\n",
        "  X_copy = MaxPool2D((2,2))(X_copy)\n",
        "  X_copy = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_copy')(X_copy)\n",
        "\n",
        "  # ADD\n",
        "  X = Add()([X,X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  # Identity Block 1\n",
        "  X_copy = X\n",
        "\n",
        "\n",
        "  # Main Path\n",
        "  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_1_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_a')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_1_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_b')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_1_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_c')(X)\n",
        "\n",
        "  # ADD\n",
        "  X = Add()([X,X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  # Identity Block 2\n",
        "  X_copy = X\n",
        "\n",
        "\n",
        "  # Main Path\n",
        "  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_2_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_a')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_2_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_b')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_2_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_c')(X)\n",
        "\n",
        "  # ADD\n",
        "  X = Add()([X,X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4wLbAgLGwLp"
      },
      "source": [
        "**RESNET - 18 block**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItbJEC358UDV"
      },
      "source": [
        "\n",
        "input_shape = (256,256,3)\n",
        "\n",
        "#Input tensor shape\n",
        "X_input = Input(input_shape)\n",
        "\n",
        "#Zero-padding\n",
        "\n",
        "X = ZeroPadding2D((3,3))(X_input)\n",
        "\n",
        "# 1 - stage\n",
        "\n",
        "X = Conv2D(64, (7,7), strides= (2,2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "X = BatchNormalization(axis =3, name = 'bn_conv1')(X)\n",
        "X = Activation('relu')(X)\n",
        "X = MaxPooling2D((3,3), strides= (2,2))(X)\n",
        "\n",
        "# 2- stage\n",
        "\n",
        "X = res_block(X, filter= [64,64,256], stage= 2)\n",
        "\n",
        "# 3- stage\n",
        "\n",
        "X = res_block(X, filter= [128,128,512], stage= 3)\n",
        "\n",
        "# 4- stage\n",
        "\n",
        "X = res_block(X, filter= [256,256,1024], stage= 4)\n",
        "\n",
        "# # 5- stage\n",
        "\n",
        "# X = res_block(X, filter= [512,512,2048], stage= 5)\n",
        "\n",
        "#Average Pooling\n",
        "\n",
        "X = AveragePooling2D((2,2), name = 'Averagea_Pooling')(X)\n",
        "\n",
        "#Final layer\n",
        "\n",
        "X = Flatten()(X)\n",
        "X = Dense(5, activation = 'softmax', name = 'Dense_final', kernel_initializer= glorot_uniform(seed=0))(X)\n",
        "\n",
        "\n",
        "model = Model( inputs= X_input, outputs = X, name = 'Resnet18')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXukzsx-KaN8"
      },
      "source": [
        "# Step - 5\n",
        "## COMPILE AND TRAIN DEEP LEARNING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxcD-7B4pii_"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics= ['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN_NR37Z07Z-"
      },
      "source": [
        "#using early stopping to exit training if validation loss is not decreasing even after certain epochs (patience)\n",
        "earlystopping = EarlyStopping(monitor='val_loss', verbose=1, patience=15)\n",
        "\n",
        "#save the best model with lower validation loss\n",
        "checkpointer = ModelCheckpoint(filepath=\"adam_patience_15.hdf5\", verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MISEltcc8XSo"
      },
      "source": [
        "history = model.fit(train_generator, steps_per_epoch = train_generator.n // 32, epochs = 2, validation_data= validation_generator, validation_steps= validation_generator.n // 32, callbacks=[checkpointer , earlystopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siUgBhJwHP0r"
      },
      "source": [
        "## Training Loss\n",
        "\n",
        "plt.plot(history.history['loss'], label='training loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend(['Training Loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOf0okyyHl4q"
      },
      "source": [
        "## Train accuracy\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.legend(['Training Accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJa65YpCHtaX"
      },
      "source": [
        "## Validation Loss\n",
        "\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Validation Loss')\n",
        "plt.legend(['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_iobL97IUDS"
      },
      "source": [
        "## Validation accuracy\n",
        "\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Validation Accuracy')\n",
        "plt.legend(['val_ccuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrcxu58dJc6D"
      },
      "source": [
        "# Step - 6\n",
        "## ASSESS THE PERFORMANCE OF THE TRAINED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Xnyn-4JgLb"
      },
      "source": [
        "model.load_weights(\"retina_weights.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T25sI_CK0X_"
      },
      "source": [
        "# Evaluate the performance of the model\n",
        "evaluate = model.evaluate(test_generator, steps = test_generator.n // 32, verbose =1)\n",
        "\n",
        "print('Accuracy Test : {}'.format(evaluate[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AsTlHqzK87m"
      },
      "source": [
        "# Assigning label names to the corresponding indexes\n",
        "labels = {0: 'Mild', 1: 'Moderate', 2: 'No_DR', 3:'Proliferate_DR', 4: 'Severe'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yZYv37SLZM3"
      },
      "source": [
        "# Loading images and their predictions \n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "# import cv2\n",
        "\n",
        "prediction = []\n",
        "original = []\n",
        "image = []\n",
        "count = 0\n",
        "\n",
        "for item in range(len(test)):\n",
        "  # code to open the image\n",
        "  img= PIL.Image.open(test['Image'].tolist()[item])\n",
        "  # resizing the image to (256,256)\n",
        "  img = img.resize((256,256))\n",
        "  # appending image to the image list\n",
        "  image.append(img)\n",
        "  # converting image to array\n",
        "  img = np.asarray(img, dtype= np.float32)\n",
        "  # normalizing the image\n",
        "  img = img / 255\n",
        "  # reshaping the image in to a 4D array\n",
        "  img = img.reshape(-1,256,256,3)\n",
        "  # making prediction of the model\n",
        "  predict = model.predict(img)\n",
        "  # getting the index corresponding to the highest value in the prediction\n",
        "  predict = np.argmax(predict)\n",
        "  # appending the predicted class to the list\n",
        "  prediction.append(labels[predict])\n",
        "  # appending original class to the list\n",
        "  original.append(test['Labels'].tolist()[item])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8l9voq4Lcm3"
      },
      "source": [
        "# Getting the test accuracy \n",
        "score = accuracy_score(original, prediction)\n",
        "print(\"Test Accuracy : {}\".format(score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HMvA198L5Xs"
      },
      "source": [
        "# Visualizing the results\n",
        "import random\n",
        "fig=plt.figure(figsize = (100,100))\n",
        "for i in range(5):\n",
        "    j = random.randint(0,len(image))\n",
        "    fig.add_subplot(20, 1, i+1)\n",
        "    plt.xlabel(\"Prediction: \" + prediction[j] +\"   Original: \" + original[j])\n",
        "    plt.imshow(image[j])\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNZXqNm4L9IE"
      },
      "source": [
        "# Print out the classification report\n",
        "print(classification_report(np.asarray(original), np.asarray(prediction)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkEl_xMrMnGs"
      },
      "source": [
        "\n",
        "# plot the confusion matrix\n",
        "plt.figure(figsize = (20,20))\n",
        "cm = confusion_matrix(np.asarray(original), np.asarray(prediction))\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(cm, annot = True, ax = ax)\n",
        "\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('Original')\n",
        "ax.set_title('Confusion_matrix')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}